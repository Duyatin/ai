{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project was build on top of the following repo: https://github.com/jwyang/faster-rcnn.pytorch\n",
    "### Modified for thyroid nodule detection\n",
    "\n",
    "###### --------------------------------------------------------\n",
    "###### Pytorch multi-GPU Faster R-CNN\n",
    "###### Licensed under The MIT License [see LICENSE for details]\n",
    "###### Written by Jiasen Lu, Jianwei Yang, based on code from Ross Girshick\n",
    "###### --------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import _init_paths\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.faster_rcnn.vgg16 import vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Called with args:\n",
      "{'batch_size': 2,\n",
      " 'class_agnostic': True,\n",
      " 'cuda': True,\n",
      " 'dataset': 'pascal_voc',\n",
      " 'disp_interval': 100,\n",
      " 'imdb_name': 'voc_2007_trainval',\n",
      " 'imdbval_name': 'voc_2007_test',\n",
      " 'large_scale': False,\n",
      " 'lr': 0.001,\n",
      " 'lr_decay_gamma': 0.333,\n",
      " 'lr_decay_step': 8,\n",
      " 'mGPUs': False,\n",
      " 'max_epochs': 1,\n",
      " 'net': 'res101',\n",
      " 'num_workers': 0,\n",
      " 'optimizer': 'sgd',\n",
      " 'resume': False,\n",
      " 'save_dir': '/home/martin/JupyterLab/output/faster_rcnn',\n",
      " 'session': 1,\n",
      " 'set_cfgs': ['ANCHOR_SCALES',\n",
      "              '[8, 16, 32]',\n",
      "              'ANCHOR_RATIOS',\n",
      "              '[0.5,1,2]',\n",
      "              'MAX_NUM_GT_BOXES',\n",
      "              '5'],\n",
      " 'start_epoch': 0,\n",
      " 'use_tfboard': False}\n",
      "/home/martin/JupyterLab/output/faster_rcnn/res101/pascal_voc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/JupyterLab/faster_rcnn_v1/model/utils/config.py:376: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "# passed\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import os\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL.Image\n",
    "import pdb\n",
    "import glob\n",
    "from torch.utils.cpp_extension import CUDA_HOME\n",
    "db_cache = glob.glob('/home/martin/JupyterLab/data/cache/*.pkl')\n",
    "if len(db_cache):\n",
    "    os.remove(db_cache[0])\n",
    "\n",
    "# Configs\n",
    "from easydict import EasyDict\n",
    "\n",
    "args = EasyDict()\n",
    "args.dataset = 'pascal_voc'\n",
    "args.net = 'res101'\n",
    "args.large_scale = False\n",
    "args.cuda = True\n",
    "args.batch_size = 2\n",
    "args.save_dir = '/home/martin/JupyterLab/output/faster_rcnn'\n",
    "args.num_workers = 0\n",
    "args.class_agnostic = True\n",
    "args.lr = 0.001\n",
    "args.optimizer = 'sgd'\n",
    "args.resume = False\n",
    "args.mGPUs = False\n",
    "args.use_tfboard = False\n",
    "args.start_epoch = 0\n",
    "args.max_epochs = 1\n",
    "args.lr_decay_step = 8\n",
    "args.lr_decay_gamma = 0.333\n",
    "args.disp_interval = 100\n",
    "args.session = 1\n",
    "args.imdb_name = \"voc_2007_trainval\"\n",
    "args.imdbval_name = \"voc_2007_test\"\n",
    "args.set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]', 'MAX_NUM_GT_BOXES', '5']\n",
    "\n",
    "print('Called with args:')\n",
    "pprint.pprint(args)\n",
    "\n",
    "# mute large_scale variable\n",
    "args.cfg_file = 'cfgs/{}.yml'.format(args.net)\n",
    "\n",
    "from model.utils.config import cfg, cfg_from_file\n",
    "if args.cfg_file is not None:\n",
    "    cfg_from_file(args.cfg_file)\n",
    "\n",
    "if torch.cuda.is_available() and not args.cuda:\n",
    "    print(\"WARNING: You have a CUDA device, so you should probably run with cuda\")\n",
    "\n",
    "cfg.CUDA = args.cuda\n",
    "cfg.USE_GPU_NMS = args.cuda\n",
    "\n",
    "# train set\n",
    "# -- Note: Use validation set and disable the flipped to enable faster loading.\n",
    "cfg.TRAIN.USE_FLIPPED = True\n",
    "cfg.POOLING_MODE = 'align'\n",
    "\n",
    "# output dir\n",
    "output_dir = args.save_dir + \"/\" + args.net + \"/\" + args.dataset\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset `voc_2007_trainval` for training\n",
      "Set proposal method: gt\n",
      "Appending horizontally-flipped training examples...\n",
      "wrote gt roidb to /home/martin/JupyterLab/data/cache/voc_2007_trainval_gt_roidb.pkl\n",
      "done\n",
      "Preparing training data...\n",
      "done\n",
      "5554 roidb entries\n"
     ]
    }
   ],
   "source": [
    "# DB helper\n",
    "def prepare_roidb(imdb):\n",
    "    \"\"\"Enrich the imdb's roidb by adding some derived quantities that\n",
    "    are useful for training. This function precomputes the maximum\n",
    "    overlap, taken over ground-truth boxes, between each ROI and\n",
    "    each ground-truth box. The class with maximum overlap is also\n",
    "    recorded.\n",
    "    \"\"\"\n",
    "    roidb = imdb.roidb\n",
    "    if not (imdb.name.startswith('coco')):\n",
    "        sizes = [PIL.Image.open(imdb.image_path_at(i)).size\n",
    "             for i in range(imdb.num_images)]\n",
    "\n",
    "    for i in range(len(imdb.image_index)):\n",
    "        roidb[i]['img_id'] = imdb.image_id_at(i)\n",
    "        roidb[i]['image'] = imdb.image_path_at(i)\n",
    "        if not (imdb.name.startswith('coco')):\n",
    "            roidb[i]['width'] = sizes[i][0]\n",
    "            roidb[i]['height'] = sizes[i][1]\n",
    "        # need gt_overlaps as a dense array for argmax\n",
    "        gt_overlaps = roidb[i]['gt_overlaps'].toarray()\n",
    "        # max overlap with gt over classes (columns)\n",
    "        max_overlaps = gt_overlaps.max(axis=1)\n",
    "        # gt class that had the max overlap\n",
    "        max_classes = gt_overlaps.argmax(axis=1)\n",
    "        roidb[i]['max_classes'] = max_classes\n",
    "        roidb[i]['max_overlaps'] = max_overlaps\n",
    "        # sanity checks\n",
    "        # max overlap of 0 => class should be zero (background)\n",
    "        zero_inds = np.where(max_overlaps == 0)[0]\n",
    "        assert all(max_classes[zero_inds] == 0)\n",
    "        # max overlap > 0 => class should not be zero (must be a fg class)\n",
    "        nonzero_inds = np.where(max_overlaps > 0)[0]\n",
    "        assert all(max_classes[nonzero_inds] != 0)\n",
    "\n",
    "def get_training_roidb(imdb):\n",
    "    \"\"\"Returns a roidb (Region of Interest database) for use in training.\"\"\"\n",
    "    if cfg.TRAIN.USE_FLIPPED:\n",
    "        print('Appending horizontally-flipped training examples...')\n",
    "        imdb.append_flipped_images()\n",
    "        print('done')\n",
    "    \n",
    "    print('Preparing training data...')\n",
    "    \n",
    "    prepare_roidb(imdb)\n",
    "    #ratio_index = rank_roidb_ratio(imdb)\n",
    "    print('done')\n",
    "    \n",
    "    return imdb.roidb\n",
    "\n",
    "# USE THYROID VOC\n",
    "from datasets.thyroid_voc import pascal_voc\n",
    "imdb = pascal_voc('trainval', '2007', '/home/martin/JupyterLab/data/VOCThyroid')\n",
    "\n",
    "print('Loaded dataset `{:s}` for training'.format(imdb.name))\n",
    "\n",
    "# data augmentation: 1x data -> 4x data\n",
    "imdb.set_proposal_method(cfg.TRAIN.PROPOSAL_METHOD)\n",
    "print('Set proposal method: {:s}'.format(cfg.TRAIN.PROPOSAL_METHOD))\n",
    "roidb = get_training_roidb(imdb)\n",
    "\n",
    "train_size = len(roidb)\n",
    "print('{:d} roidb entries'.format(len(roidb)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering, there are 5554 images...\n",
      "after filtering, there are 5554 images...\n"
     ]
    }
   ],
   "source": [
    "def filter_roidb(roidb):\n",
    "    # filter the image without bounding box.\n",
    "    print('before filtering, there are %d images...' % (len(roidb)))\n",
    "    i = 0\n",
    "    while i < len(roidb):\n",
    "        if len(roidb[i]['boxes']) == 0:\n",
    "            del roidb[i]\n",
    "            i -= 1\n",
    "        i += 1\n",
    "    \n",
    "    print('after filtering, there are %d images...' % (len(roidb)))\n",
    "    return roidb\n",
    "\n",
    "def rank_roidb_ratio(roidb):\n",
    "    # rank roidb based on the ratio between width and height.\n",
    "    ratio_large = 2 # largest ratio to preserve.\n",
    "    ratio_small = 0.5 # smallest ratio to preserve.    \n",
    "    \n",
    "    ratio_list = []\n",
    "    for i in range(len(roidb)):\n",
    "        width = roidb[i]['width']\n",
    "        height = roidb[i]['height']\n",
    "        ratio = width / float(height)\n",
    "\n",
    "        if ratio > ratio_large:\n",
    "            roidb[i]['need_crop'] = 1\n",
    "            ratio = ratio_large\n",
    "        elif ratio < ratio_small:\n",
    "            roidb[i]['need_crop'] = 1\n",
    "            ratio = ratio_small        \n",
    "        else:\n",
    "            roidb[i]['need_crop'] = 0\n",
    "\n",
    "        ratio_list.append(ratio)\n",
    "\n",
    "    ratio_list = np.array(ratio_list)\n",
    "    ratio_index = np.argsort(ratio_list)\n",
    "    return ratio_list[ratio_index], ratio_index\n",
    "\n",
    "from torch.utils.data.sampler import Sampler\n",
    "class sampler(Sampler):\n",
    "    def __init__(self, train_size, batch_size):\n",
    "        self.num_data = train_size\n",
    "        self.num_per_batch = int(train_size / batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.range = torch.arange(0,batch_size).view(1, batch_size).long()\n",
    "        self.leftover_flag = False\n",
    "        if train_size % batch_size:\n",
    "            self.leftover = torch.arange(self.num_per_batch*batch_size, train_size).long()\n",
    "            self.leftover_flag = True\n",
    "\n",
    "    def __iter__(self):\n",
    "        rand_num = torch.randperm(self.num_per_batch).view(-1,1) * self.batch_size\n",
    "        self.rand_num = rand_num.expand(self.num_per_batch, self.batch_size) + self.range\n",
    "\n",
    "        self.rand_num_view = self.rand_num.view(-1)\n",
    "\n",
    "        if self.leftover_flag:\n",
    "            self.rand_num_view = torch.cat((self.rand_num_view, self.leftover),0)\n",
    "\n",
    "        return iter(self.rand_num_view)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_data\n",
    "\n",
    "sampler_batch = sampler(train_size, args.batch_size)\n",
    "\n",
    "roidb = filter_roidb(roidb)\n",
    "ratio_list, ratio_index = rank_roidb_ratio(roidb)\n",
    "\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from importlib import reload\n",
    "import sys\n",
    "roibatchLoader=reload(sys.modules['roi_data_layer.roibatchLoader']).roibatchLoader\n",
    "dataset = roibatchLoader(\n",
    "    roidb, \n",
    "    ratio_list, \n",
    "    ratio_index, \n",
    "    args.batch_size, \n",
    "    imdb.num_classes, \n",
    "    training=True\n",
    ")\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, \n",
    "    batch_size=args.batch_size,\n",
    "    sampler=sampler_batch, \n",
    "    num_workers=args.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained weights from data/pretrained_model/resnet101_caffe.pth\n"
     ]
    }
   ],
   "source": [
    "from model.faster_rcnn.resnet import resnet\n",
    "\n",
    "# initilize the network here.\n",
    "fasterRCNN = resnet(imdb.classes, 101, pretrained=True, class_agnostic=args.class_agnostic)\n",
    "\n",
    "fasterRCNN.create_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint /home/martin/JupyterLab/output/faster_rcnn/res101/pascal_voc/faster_rcnn_1_73_2776.pth\n",
      "load model successfully!\n",
      "Ship faster RCNN to cuda\n"
     ]
    }
   ],
   "source": [
    "###########################################################################\n",
    "##### Load from checkpoint\n",
    "###########################################################################\n",
    "input_dir = '/home/martin/JupyterLab/output/faster_rcnn/res101/pascal_voc/'\n",
    "load_name = input_dir + 'faster_rcnn_1_73_2776.pth'\n",
    "\n",
    "print(\"load checkpoint %s\" % (load_name))\n",
    "if args.cuda > 0:\n",
    "    checkpoint = torch.load(load_name)\n",
    "args.session = checkpoint['session']\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "lr = optimizer.param_groups[0]['lr']\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']\n",
    "    \n",
    "print('load model successfully!')\n",
    "\n",
    "fasterRCNN.cuda()\n",
    "print('Ship faster RCNN to cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martin/JupyterLab/faster_rcnn_v1/venv/lib/python3.7/site-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/martin/JupyterLab/faster_rcnn_v1/venv/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  app.launch_new_instance()\n",
      "/home/martin/JupyterLab/faster_rcnn_v1/venv/lib/python3.7/site-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/martin/JupyterLab/faster_rcnn_v1/venv/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
     ]
    }
   ],
   "source": [
    "# Specify class information\n",
    "pascal_classes = np.asarray(['__background__', 'lesion'])\n",
    "\n",
    "# initilize the tensor holder here.\n",
    "im_data = torch.FloatTensor(1)\n",
    "im_info = torch.FloatTensor(1)\n",
    "num_boxes = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)\n",
    "\n",
    "# ship to cuda\n",
    "if args.cuda > 0:\n",
    "    im_data = im_data.cuda()\n",
    "    im_info = im_info.cuda()\n",
    "    num_boxes = num_boxes.cuda()\n",
    "    gt_boxes = gt_boxes.cuda()\n",
    "\n",
    "# make variable\n",
    "im_data = Variable(im_data, volatile=True)\n",
    "im_info = Variable(im_info, volatile=True)\n",
    "num_boxes = Variable(num_boxes, volatile=True)\n",
    "gt_boxes = Variable(gt_boxes, volatile=True)\n",
    "\n",
    "fasterRCNN.eval()\n",
    "\n",
    "start = time.time()\n",
    "max_per_image = 100\n",
    "thresh = 0.05\n",
    "vis = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from model.utils.blob import im_list_to_blob\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "from model.roi_layers import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from matplotlib.pyplot import imread\n",
    "\n",
    "def _get_image_blob(im):\n",
    "    \"\"\"Converts an image into a network input.\n",
    "    Arguments:\n",
    "    im (ndarray): a color image in BGR order\n",
    "    Returns:\n",
    "    blob (ndarray): a data blob holding an image pyramid\n",
    "    im_scale_factors (list): list of image scales (relative to im) used\n",
    "      in the image pyramid\n",
    "    \"\"\"\n",
    "    im_orig = im.astype(np.float32, copy=True)\n",
    "    im_orig -= cfg.PIXEL_MEANS\n",
    "\n",
    "    im_shape = im_orig.shape\n",
    "    im_size_min = np.min(im_shape[0:2])\n",
    "    im_size_max = np.max(im_shape[0:2])\n",
    "\n",
    "    processed_ims = []\n",
    "    im_scale_factors = []\n",
    "\n",
    "    for target_size in cfg.TEST.SCALES:\n",
    "        im_scale = float(target_size) / float(im_size_min)\n",
    "        # Prevent the biggest axis from being more than MAX_SIZE\n",
    "        if np.round(im_scale * im_size_max) > cfg.TEST.MAX_SIZE:\n",
    "              im_scale = float(cfg.TEST.MAX_SIZE) / float(im_size_max)\n",
    "        im = cv2.resize(im_orig, None, None, fx=im_scale, fy=im_scale,\n",
    "                interpolation=cv2.INTER_LINEAR)\n",
    "        im_scale_factors.append(im_scale)\n",
    "        processed_ims.append(im)\n",
    "\n",
    "    # Create a blob to hold the input images\n",
    "    blob = im_list_to_blob(processed_ims)\n",
    "\n",
    "    return blob, np.array(im_scale_factors)\n",
    "\n",
    "def vis_detections(im, class_name, dets, thresh=0.8):\n",
    "    \"\"\"Visual debugging of detections.\"\"\"\n",
    "    for i in range(np.minimum(10, dets.shape[0])):\n",
    "        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n",
    "        score = dets[i, -1]\n",
    "        if score > thresh:\n",
    "            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 204, 0), 2)\n",
    "            cv2.putText(im, '%s: %.3f' % (class_name, score), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n",
    "                        1.0, (0, 0, 255), thickness=1)\n",
    "    return im\n",
    "\n",
    "def get_gt_bbox(img_filename):\n",
    "    # e.g. '117.png'\n",
    "    annotation_path = '/home/martin/JupyterLab/data/VOCThyroid/VOC2007/Annotations/'\n",
    "    annotation_file = annotation_path + img_filename + '.xml'\n",
    "    \n",
    "    import xml.etree.ElementTree as ET\n",
    "    tree = ET.parse(annotation_file)\n",
    "    objs = tree.findall('object')\n",
    "    \n",
    "    bbox_list = []\n",
    "    for ix, obj in enumerate(objs):\n",
    "        bbox = obj.find('bndbox')\n",
    "        # Make pixel indexes 0-based\n",
    "        x1 = int(bbox.find('xmin').text)\n",
    "        y1 = int(bbox.find('ymin').text)\n",
    "        x2 = int(bbox.find('xmax').text)\n",
    "        y2 = int(bbox.find('ymax').text)\n",
    "        bbox_list.append({\n",
    "            'bbox': (x1,y1,x2,y2),\n",
    "            'text': 'gt'\n",
    "        })\n",
    "        \n",
    "    return(bbox_list)\n",
    "\n",
    "def get_pred_bbox(class_name, dets, thresh=0.8):\n",
    "    # class_name = pascal_classes[j]\n",
    "    # dets = cls_dets.cpu().numpy()\n",
    "    # thresh = 0.3\n",
    "    \n",
    "    bbox_list = [] \n",
    "    # Get bbox for score > threshold\n",
    "    for i in range(np.minimum(10, dets.shape[0])):\n",
    "        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n",
    "        score = dets[i, -1]\n",
    "        if score > thresh:\n",
    "            bbox_list.append({\n",
    "                'bbox': bbox,\n",
    "                'score': score, \n",
    "                'text': '%s: %.3f' % (class_name, score) \n",
    "            })\n",
    "    \n",
    "    # If no score > threshold, get bbox for highest score\n",
    "    if len(bbox_list) == 0:\n",
    "        dets = dets[dets[:,-1].argsort()]\n",
    "        bbox = tuple(int(np.round(x)) for x in dets[-1, :4])\n",
    "        score = dets[-1, -1]\n",
    "        bbox_list.append({\n",
    "            'bbox': bbox,\n",
    "            'score': score, \n",
    "            'text': '%s: %.3f' % (class_name, score) \n",
    "        })\n",
    "    \n",
    "    return(bbox_list)\n",
    "\n",
    "def vis_bbox(im, bbox_list, color = (255, 0, 0)):\n",
    "    for bbox_obj in bbox_list:\n",
    "        bbox = bbox_obj['bbox']\n",
    "        text = bbox_obj['text']\n",
    "        \n",
    "        cv2.rectangle(im, bbox[0:2], bbox[2:4], color, 2)\n",
    "        cv2.putText(im, text, (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.0, color, thickness=1)\n",
    "    \n",
    "    return im\n",
    "\n",
    "def get_iou(bb1_raw, bb2_raw):\n",
    "    bb1 = {\n",
    "        'x1': bb1_raw[0],\n",
    "        'y1': bb1_raw[1],\n",
    "        'x2': bb1_raw[2],\n",
    "        'y2': bb1_raw[3],\n",
    "    }\n",
    "    \n",
    "    bb2 = {\n",
    "        'x1': bb2_raw[0],\n",
    "        'y1': bb2_raw[1],\n",
    "        'x2': bb2_raw[2],\n",
    "        'y2': bb2_raw[3],\n",
    "    }\n",
    "    \n",
    "    assert bb1['x1'] < bb1['x2']\n",
    "    assert bb1['y1'] < bb1['y2']\n",
    "    assert bb2['x1'] < bb2['x2']\n",
    "    assert bb2['y1'] < bb2['y2']\n",
    "\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    x_left = max(bb1['x1'], bb2['x1'])\n",
    "    y_top = max(bb1['y1'], bb2['y1'])\n",
    "    x_right = min(bb1['x2'], bb2['x2'])\n",
    "    y_bottom = min(bb1['y2'], bb2['y2'])\n",
    "\n",
    "    if x_right < x_left or y_bottom < y_top:\n",
    "        return 0.0\n",
    "\n",
    "    # The intersection of two axis-aligned bounding boxes is always an\n",
    "    # axis-aligned bounding box\n",
    "    intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
    "\n",
    "    # compute the area of both AABBs\n",
    "    bb1_area = (bb1['x2'] - bb1['x1']) * (bb1['y2'] - bb1['y1'])\n",
    "    bb2_area = (bb2['x2'] - bb2['x1']) * (bb2['y2'] - bb2['y1'])\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
    "    assert iou >= 0.0\n",
    "    assert iou <= 1.0\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# DEMO MODE: LOAD IMG FROM DIR\n",
    "args.image_dir = '/home/martin/JupyterLab/data/VOCThyroid/VOC2007/JPEGImages/'\n",
    "\n",
    "# Load test.txt and combine into '[ID].png'\n",
    "f_path = '/home/martin/JupyterLab/data/VOCThyroid/VOC2007/ImageSets/Main/'\n",
    "f = open(f_path + 'test.txt', 'r')\n",
    "imglist = f.readlines()\n",
    "f.close()\n",
    "imglist = list(map(lambda x: str(int(x)) + '.png', imglist))\n",
    "\n",
    "imglist = imglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Photo: 299 images.\n",
      "im_detect: 299/299 0.095s 0.001s   \n",
      "im_detect: 298/299 0.101s 0.002s   \n",
      "im_detect: 297/299 0.101s 0.004s   \n",
      "im_detect: 296/299 0.098s 0.001s   \n",
      "im_detect: 295/299 0.100s 0.003s   \n",
      "im_detect: 294/299 0.103s 0.004s   \n",
      "im_detect: 293/299 0.098s 0.007s   \n",
      "im_detect: 292/299 0.108s 0.002s   \n",
      "im_detect: 291/299 0.103s 0.005s   \n",
      "im_detect: 290/299 0.104s 0.001s   \n",
      "im_detect: 289/299 0.099s 0.002s   \n",
      "im_detect: 288/299 0.099s 0.003s   \n",
      "im_detect: 287/299 0.098s 0.004s   \n",
      "im_detect: 286/299 0.100s 0.004s   \n",
      "im_detect: 285/299 0.099s 0.007s   \n",
      "im_detect: 284/299 0.098s 0.004s   \n",
      "im_detect: 283/299 0.099s 0.004s   \n",
      "im_detect: 282/299 0.100s 0.004s   \n",
      "im_detect: 281/299 0.099s 0.004s   \n",
      "im_detect: 280/299 0.099s 0.004s   \n",
      "im_detect: 279/299 0.097s 0.005s   \n",
      "im_detect: 278/299 0.099s 0.008s   \n",
      "im_detect: 277/299 0.104s 0.002s   \n",
      "im_detect: 276/299 0.098s 0.004s   \n",
      "im_detect: 275/299 0.104s 0.001s   \n",
      "im_detect: 274/299 0.100s 0.002s   \n",
      "im_detect: 273/299 0.097s 0.007s   \n",
      "im_detect: 272/299 0.099s 0.004s   \n",
      "im_detect: 271/299 0.099s 0.003s   \n",
      "im_detect: 270/299 0.098s 0.003s   \n",
      "im_detect: 269/299 0.099s 0.004s   \n",
      "im_detect: 268/299 0.098s 0.003s   \n",
      "im_detect: 267/299 0.098s 0.004s   \n",
      "im_detect: 266/299 0.097s 0.004s   \n",
      "im_detect: 265/299 0.097s 0.003s   \n",
      "im_detect: 264/299 0.106s 0.007s   \n",
      "im_detect: 263/299 0.099s 0.004s   \n",
      "im_detect: 262/299 0.105s 0.002s   \n",
      "im_detect: 261/299 0.099s 0.004s   \n",
      "im_detect: 260/299 0.096s 0.007s   \n",
      "im_detect: 259/299 0.101s 0.002s   \n",
      "im_detect: 258/299 0.098s 0.004s   \n",
      "im_detect: 257/299 0.098s 0.004s   \n",
      "im_detect: 256/299 0.097s 0.002s   \n",
      "im_detect: 255/299 0.099s 0.004s   \n",
      "im_detect: 254/299 0.100s 0.004s   \n",
      "im_detect: 253/299 0.102s 0.002s   \n",
      "im_detect: 252/299 0.103s 0.001s   \n",
      "im_detect: 251/299 0.100s 0.004s   \n",
      "im_detect: 250/299 0.101s 0.002s   \n",
      "im_detect: 249/299 0.098s 0.007s   \n",
      "im_detect: 248/299 0.099s 0.004s   \n",
      "im_detect: 247/299 0.102s 0.002s   \n",
      "im_detect: 246/299 0.098s 0.004s   \n",
      "im_detect: 245/299 0.101s 0.001s   \n",
      "im_detect: 244/299 0.101s 0.003s   \n",
      "im_detect: 243/299 0.099s 0.004s   \n",
      "im_detect: 242/299 0.101s 0.001s   \n",
      "im_detect: 241/299 0.096s 0.003s   \n",
      "im_detect: 240/299 0.102s 0.001s   \n",
      "im_detect: 239/299 0.104s 0.001s   \n",
      "im_detect: 238/299 0.096s 0.002s   \n",
      "im_detect: 237/299 0.100s 0.004s   \n",
      "im_detect: 236/299 0.095s 0.004s   \n",
      "im_detect: 235/299 0.099s 0.003s   \n",
      "im_detect: 234/299 0.099s 0.003s   \n",
      "im_detect: 233/299 0.100s 0.004s   \n",
      "im_detect: 232/299 0.101s 0.003s   \n",
      "im_detect: 231/299 0.099s 0.005s   \n",
      "im_detect: 230/299 0.098s 0.004s   \n",
      "im_detect: 229/299 0.099s 0.003s   \n",
      "im_detect: 228/299 0.099s 0.004s   \n",
      "im_detect: 227/299 0.097s 0.004s   \n",
      "im_detect: 226/299 0.098s 0.003s   \n",
      "im_detect: 225/299 0.102s 0.004s   \n",
      "im_detect: 224/299 0.101s 0.004s   \n",
      "im_detect: 223/299 0.097s 0.002s   \n",
      "im_detect: 222/299 0.099s 0.004s   \n",
      "im_detect: 221/299 0.100s 0.002s   \n",
      "im_detect: 220/299 0.105s 0.002s   \n",
      "im_detect: 219/299 0.099s 0.004s   \n",
      "im_detect: 218/299 0.098s 0.005s   \n",
      "im_detect: 217/299 0.100s 0.002s   \n",
      "im_detect: 216/299 0.101s 0.008s   \n",
      "im_detect: 215/299 0.099s 0.004s   \n",
      "im_detect: 214/299 0.100s 0.004s   \n",
      "im_detect: 213/299 0.096s 0.004s   \n",
      "im_detect: 212/299 0.100s 0.005s   \n",
      "im_detect: 211/299 0.102s 0.003s   \n",
      "im_detect: 210/299 0.101s 0.004s   \n",
      "im_detect: 209/299 0.101s 0.004s   \n",
      "im_detect: 208/299 0.096s 0.004s   \n",
      "im_detect: 207/299 0.097s 0.004s   \n",
      "im_detect: 206/299 0.098s 0.003s   \n",
      "im_detect: 205/299 0.104s 0.002s   \n",
      "im_detect: 204/299 0.100s 0.004s   \n",
      "im_detect: 203/299 0.096s 0.003s   \n",
      "im_detect: 202/299 0.099s 0.002s   \n",
      "im_detect: 201/299 0.100s 0.003s   \n",
      "im_detect: 200/299 0.098s 0.004s   \n",
      "im_detect: 199/299 0.101s 0.002s   \n",
      "im_detect: 198/299 0.097s 0.004s   \n",
      "im_detect: 197/299 0.100s 0.004s   \n",
      "im_detect: 196/299 0.098s 0.004s   \n",
      "im_detect: 195/299 0.100s 0.004s   \n",
      "im_detect: 194/299 0.096s 0.004s   \n",
      "im_detect: 193/299 0.102s 0.003s   \n",
      "im_detect: 192/299 0.099s 0.004s   \n",
      "im_detect: 191/299 0.102s 0.001s   \n",
      "im_detect: 190/299 0.098s 0.002s   \n",
      "im_detect: 189/299 0.097s 0.004s   \n",
      "im_detect: 188/299 0.098s 0.004s   \n",
      "im_detect: 187/299 0.099s 0.003s   \n",
      "im_detect: 186/299 0.099s 0.005s   \n",
      "im_detect: 185/299 0.097s 0.004s   \n",
      "im_detect: 184/299 0.103s 0.002s   \n",
      "im_detect: 183/299 0.097s 0.003s   \n",
      "im_detect: 182/299 0.102s 0.004s   \n",
      "im_detect: 181/299 0.102s 0.001s   \n",
      "im_detect: 180/299 0.097s 0.004s   \n",
      "im_detect: 179/299 0.097s 0.005s   \n",
      "im_detect: 178/299 0.099s 0.002s   \n",
      "im_detect: 177/299 0.099s 0.004s   \n",
      "im_detect: 176/299 0.099s 0.001s   \n",
      "im_detect: 175/299 0.102s 0.003s   \n",
      "im_detect: 174/299 0.099s 0.003s   \n",
      "im_detect: 173/299 0.100s 0.004s   \n",
      "im_detect: 172/299 0.100s 0.008s   \n",
      "im_detect: 171/299 0.097s 0.004s   \n",
      "im_detect: 170/299 0.097s 0.005s   \n",
      "im_detect: 169/299 0.100s 0.003s   \n",
      "im_detect: 168/299 0.100s 0.004s   \n",
      "im_detect: 167/299 0.099s 0.007s   \n",
      "im_detect: 166/299 0.100s 0.002s   \n",
      "im_detect: 165/299 0.103s 0.001s   \n",
      "im_detect: 164/299 0.098s 0.004s   \n",
      "im_detect: 163/299 0.100s 0.003s   \n",
      "im_detect: 162/299 0.099s 0.004s   \n",
      "im_detect: 161/299 0.099s 0.004s   \n",
      "im_detect: 160/299 0.101s 0.003s   \n",
      "im_detect: 159/299 0.101s 0.004s   \n",
      "im_detect: 158/299 0.104s 0.002s   \n",
      "im_detect: 157/299 0.098s 0.007s   \n",
      "im_detect: 156/299 0.095s 0.009s   \n",
      "im_detect: 155/299 0.098s 0.004s   \n",
      "im_detect: 154/299 0.111s 0.002s   \n",
      "im_detect: 153/299 0.100s 0.002s   \n",
      "im_detect: 152/299 0.102s 0.004s   \n",
      "im_detect: 151/299 0.099s 0.002s   \n",
      "im_detect: 150/299 0.099s 0.004s   \n",
      "im_detect: 149/299 0.100s 0.005s   \n",
      "im_detect: 148/299 0.099s 0.004s   \n",
      "im_detect: 147/299 0.099s 0.004s   \n",
      "im_detect: 146/299 0.098s 0.002s   \n",
      "im_detect: 145/299 0.105s 0.002s   \n",
      "im_detect: 144/299 0.100s 0.009s   \n",
      "im_detect: 143/299 0.100s 0.002s   \n",
      "im_detect: 142/299 0.098s 0.003s   \n",
      "im_detect: 141/299 0.101s 0.008s   \n",
      "im_detect: 140/299 0.099s 0.004s   \n",
      "im_detect: 139/299 0.098s 0.005s   \n",
      "im_detect: 138/299 0.101s 0.006s   \n",
      "im_detect: 137/299 0.098s 0.005s   \n",
      "im_detect: 136/299 0.098s 0.002s   \n",
      "im_detect: 135/299 0.098s 0.004s   \n",
      "im_detect: 134/299 0.100s 0.004s   \n",
      "im_detect: 133/299 0.100s 0.005s   \n",
      "im_detect: 132/299 0.097s 0.004s   \n",
      "im_detect: 131/299 0.100s 0.004s   \n",
      "im_detect: 130/299 0.100s 0.002s   \n",
      "im_detect: 129/299 0.099s 0.004s   \n",
      "im_detect: 128/299 0.102s 0.005s   \n",
      "im_detect: 127/299 0.097s 0.002s   \n",
      "im_detect: 126/299 0.103s 0.004s   \n",
      "im_detect: 125/299 0.100s 0.008s   \n",
      "im_detect: 124/299 0.106s 0.002s   \n",
      "im_detect: 123/299 0.101s 0.004s   \n",
      "im_detect: 122/299 0.100s 0.005s   \n",
      "im_detect: 121/299 0.097s 0.002s   \n",
      "im_detect: 120/299 0.099s 0.004s   \n",
      "im_detect: 119/299 0.099s 0.005s   \n",
      "im_detect: 118/299 0.101s 0.002s   \n",
      "im_detect: 117/299 0.096s 0.004s   \n",
      "im_detect: 116/299 0.097s 0.001s   \n",
      "im_detect: 115/299 0.103s 0.003s   \n",
      "im_detect: 114/299 0.099s 0.012s   \n",
      "im_detect: 113/299 0.100s 0.005s   \n",
      "im_detect: 112/299 0.102s 0.003s   \n",
      "im_detect: 111/299 0.099s 0.004s   \n",
      "im_detect: 110/299 0.099s 0.004s   \n",
      "im_detect: 109/299 0.099s 0.004s   \n",
      "im_detect: 108/299 0.098s 0.004s   \n",
      "im_detect: 107/299 0.097s 0.004s   \n",
      "im_detect: 106/299 0.098s 0.004s   \n",
      "im_detect: 105/299 0.099s 0.004s   \n",
      "im_detect: 104/299 0.101s 0.004s   \n",
      "im_detect: 103/299 0.099s 0.001s   \n",
      "im_detect: 102/299 0.099s 0.004s   \n",
      "im_detect: 101/299 0.101s 0.004s   \n",
      "im_detect: 100/299 0.102s 0.003s   \n",
      "im_detect: 99/299 0.099s 0.004s   \n",
      "im_detect: 98/299 0.097s 0.004s   \n",
      "im_detect: 97/299 0.103s 0.004s   \n",
      "im_detect: 96/299 0.098s 0.004s   \n",
      "im_detect: 95/299 0.101s 0.004s   \n",
      "im_detect: 94/299 0.099s 0.004s   \n",
      "im_detect: 93/299 0.097s 0.004s   \n",
      "im_detect: 92/299 0.099s 0.007s   \n",
      "im_detect: 91/299 0.107s 0.002s   \n",
      "im_detect: 90/299 0.097s 0.003s   \n",
      "im_detect: 89/299 0.097s 0.005s   \n",
      "im_detect: 88/299 0.098s 0.003s   \n",
      "im_detect: 87/299 0.099s 0.004s   \n",
      "im_detect: 86/299 0.099s 0.005s   \n",
      "im_detect: 85/299 0.100s 0.002s   \n",
      "im_detect: 84/299 0.098s 0.001s   \n",
      "im_detect: 83/299 0.096s 0.004s   \n",
      "im_detect: 82/299 0.102s 0.002s   \n",
      "im_detect: 81/299 0.102s 0.009s   \n",
      "im_detect: 80/299 0.100s 0.004s   \n",
      "im_detect: 79/299 0.097s 0.002s   \n",
      "im_detect: 78/299 0.100s 0.004s   \n",
      "im_detect: 77/299 0.100s 0.004s   \n",
      "im_detect: 76/299 0.100s 0.006s   \n",
      "im_detect: 75/299 0.102s 0.003s   \n",
      "im_detect: 74/299 0.095s 0.001s   \n",
      "im_detect: 73/299 0.108s 0.002s   \n",
      "im_detect: 72/299 0.097s 0.004s   \n",
      "im_detect: 71/299 0.102s 0.004s   \n",
      "im_detect: 70/299 0.103s 0.002s   \n",
      "im_detect: 69/299 0.097s 0.004s   \n",
      "im_detect: 68/299 0.099s 0.004s   \n",
      "im_detect: 67/299 0.100s 0.004s   \n",
      "im_detect: 66/299 0.102s 0.004s   \n",
      "im_detect: 65/299 0.100s 0.004s   \n",
      "im_detect: 64/299 0.095s 0.005s   \n",
      "im_detect: 63/299 0.099s 0.004s   \n",
      "im_detect: 62/299 0.099s 0.004s   \n",
      "im_detect: 61/299 0.102s 0.003s   \n",
      "im_detect: 60/299 0.098s 0.004s   \n",
      "im_detect: 59/299 0.097s 0.001s   \n",
      "im_detect: 58/299 0.100s 0.003s   \n",
      "im_detect: 57/299 0.098s 0.005s   \n",
      "im_detect: 56/299 0.100s 0.004s   \n",
      "im_detect: 55/299 0.099s 0.002s   \n",
      "im_detect: 54/299 0.099s 0.005s   \n",
      "im_detect: 53/299 0.102s 0.004s   \n",
      "im_detect: 52/299 0.102s 0.002s   \n",
      "im_detect: 51/299 0.101s 0.004s   \n",
      "im_detect: 50/299 0.100s 0.004s   \n",
      "im_detect: 49/299 0.098s 0.005s   \n",
      "im_detect: 48/299 0.099s 0.005s   \n",
      "im_detect: 47/299 0.099s 0.004s   \n",
      "im_detect: 46/299 0.100s 0.004s   \n",
      "im_detect: 45/299 0.095s 0.005s   \n",
      "im_detect: 44/299 0.100s 0.004s   \n",
      "im_detect: 43/299 0.100s 0.008s   \n",
      "im_detect: 42/299 0.100s 0.004s   \n",
      "im_detect: 41/299 0.100s 0.004s   \n",
      "im_detect: 40/299 0.098s 0.002s   \n",
      "im_detect: 39/299 0.098s 0.007s   \n",
      "im_detect: 38/299 0.103s 0.004s   \n",
      "im_detect: 37/299 0.100s 0.003s   \n",
      "im_detect: 36/299 0.101s 0.004s   \n",
      "im_detect: 35/299 0.104s 0.001s   \n",
      "im_detect: 34/299 0.099s 0.005s   \n",
      "im_detect: 33/299 0.099s 0.007s   \n",
      "im_detect: 32/299 0.104s 0.001s   \n",
      "im_detect: 31/299 0.104s 0.002s   \n",
      "im_detect: 30/299 0.099s 0.001s   \n",
      "im_detect: 29/299 0.102s 0.005s   \n",
      "im_detect: 28/299 0.106s 0.004s   \n",
      "im_detect: 27/299 0.102s 0.004s   \n",
      "im_detect: 26/299 0.099s 0.001s   \n",
      "im_detect: 25/299 0.096s 0.002s   \n",
      "im_detect: 24/299 0.108s 0.004s   \n",
      "im_detect: 23/299 0.099s 0.004s   \n",
      "im_detect: 22/299 0.098s 0.003s   \n",
      "im_detect: 21/299 0.099s 0.005s   \n",
      "im_detect: 20/299 0.096s 0.004s   \n",
      "im_detect: 19/299 0.100s 0.004s   \n",
      "im_detect: 18/299 0.100s 0.004s   \n",
      "im_detect: 17/299 0.098s 0.005s   \n",
      "im_detect: 16/299 0.100s 0.002s   \n",
      "im_detect: 15/299 0.095s 0.004s   \n",
      "im_detect: 14/299 0.100s 0.003s   \n",
      "im_detect: 13/299 0.099s 0.003s   \n",
      "im_detect: 12/299 0.100s 0.004s   \n",
      "im_detect: 11/299 0.097s 0.006s   \n",
      "im_detect: 10/299 0.106s 0.002s   \n",
      "im_detect: 9/299 0.099s 0.004s   \n",
      "im_detect: 8/299 0.100s 0.005s   \n",
      "im_detect: 7/299 0.101s 0.002s   \n",
      "im_detect: 6/299 0.096s 0.004s   \n",
      "im_detect: 5/299 0.097s 0.008s   \n",
      "im_detect: 4/299 0.102s 0.003s   \n",
      "im_detect: 3/299 0.111s 0.002s   \n",
      "im_detect: 2/299 0.099s 0.004s   \n",
      "im_detect: 1/299 0.095s 0.004s   \n"
     ]
    }
   ],
   "source": [
    "######################### Loop code ###########################\n",
    "iou_list = []\n",
    "vis = 1\n",
    "num_images = len(imglist)\n",
    "print('Loaded Photo: {} images.'.format(num_images))\n",
    "\n",
    "for i in range(num_images):\n",
    "    \n",
    "    total_tic = time.time()\n",
    "    num_images -= 1\n",
    "    \n",
    "    im_file = os.path.join(args.image_dir, imglist[num_images])\n",
    "    # im = cv2.imread(im_file)\n",
    "    im_in = np.array(imread(im_file))\n",
    "    # rgb -> bgr\n",
    "    im = im_in[:,:,::-1]\n",
    "    # im_scales is scale ratio\n",
    "    blobs, im_scales = _get_image_blob(im)\n",
    "    assert len(im_scales) == 1, \"Only single-image batch implemented\"\n",
    "    \n",
    "    im_blob = blobs\n",
    "    # im_info_np is height, width, scale ratio\n",
    "    im_info_np = np.array([[im_blob.shape[1], im_blob.shape[2], im_scales[0]]], dtype=np.float32)\n",
    "    \n",
    "    # convert to pytorch and permute color channel\n",
    "    im_data_pt = torch.from_numpy(im_blob)\n",
    "    im_data_pt = im_data_pt.permute(0, 3, 1, 2)\n",
    "    im_info_pt = torch.from_numpy(im_info_np)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        im_data.resize_(im_data_pt.size()).copy_(im_data_pt)\n",
    "        im_info.resize_(im_info_pt.size()).copy_(im_info_pt)\n",
    "        gt_boxes.resize_(1, 1, 5).zero_()\n",
    "        num_boxes.resize_(1).zero_()\n",
    "        \n",
    "    det_tic = time.time()\n",
    "    \n",
    "    rois, cls_prob, bbox_pred, \\\n",
    "    rpn_loss_cls, rpn_loss_box, \\\n",
    "    RCNN_loss_cls, RCNN_loss_bbox, \\\n",
    "    rois_label = fasterRCNN(im_data, im_info, gt_boxes, num_boxes)\n",
    "    \n",
    "    # Here: box = drop first column of rois \n",
    "    scores = cls_prob.data\n",
    "    boxes = rois.data[:, :, 1:5]\n",
    "    \n",
    "    # Apply bounding-box regression deltas\n",
    "    box_deltas = bbox_pred.data\n",
    "    if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "    # Optionally normalize targets by a precomputed mean and stdev\n",
    "        if args.class_agnostic:\n",
    "            if args.cuda > 0:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "            else:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS) \\\n",
    "                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n",
    "    \n",
    "            box_deltas = box_deltas.view(1, -1, 4)\n",
    "        else:\n",
    "            if args.cuda > 0:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "            else:\n",
    "                box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS) \\\n",
    "                           + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n",
    "            box_deltas = box_deltas.view(1, -1, 4 * len(pascal_classes))\n",
    "    \n",
    "    pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "    pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n",
    "    \n",
    "    pred_boxes /= im_scales[0]\n",
    "    \n",
    "    scores = scores.squeeze()\n",
    "    pred_boxes = pred_boxes.squeeze()\n",
    "    det_toc = time.time()\n",
    "    detect_time = det_toc - det_tic\n",
    "    misc_tic = time.time()\n",
    "    if vis:\n",
    "        im2show = np.copy(im)\n",
    "        # This pixel*255 operation is replaced by cv2.convertScaleAbs(im2show, alpha=(255.0))\n",
    "        #im2show = im2show*255\n",
    "    \n",
    "    ####\n",
    "    ## bbox gen\n",
    "    ####\n",
    "    # Loop through pascal classes and inference\n",
    "    for j in range(1, len(pascal_classes)):\n",
    "        # get all indices of score of j class exceeding threshold\n",
    "        inds = torch.nonzero(scores[:,j]>thresh).view(-1)\n",
    "        \n",
    "        # visuzalize gt box\n",
    "        img_filename = (imglist[num_images]).replace('.png', '')\n",
    "        gt_bbox_list = get_gt_bbox(img_filename)\n",
    "        if vis:       \n",
    "            im2show = vis_bbox(im2show, gt_bbox_list, (0,255,0))\n",
    "        \n",
    "        # if there is det\n",
    "        if inds.numel() > 0:\n",
    "            cls_scores = scores[:,j][inds]\n",
    "            _, order = torch.sort(cls_scores, 0, True)\n",
    "            if args.class_agnostic:\n",
    "                cls_boxes = pred_boxes[inds, :]\n",
    "            else:\n",
    "                cls_boxes = pred_boxes[inds][:, j * 4:(j + 1) * 4]\n",
    "            \n",
    "            cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "            # cls_dets = torch.cat((cls_boxes, cls_scores), 1)\n",
    "            cls_dets = cls_dets[order]\n",
    "            # keep = nms(cls_dets, cfg.TEST.NMS, force_cpu=not cfg.USE_GPU_NMS)\n",
    "            keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n",
    "            cls_dets = cls_dets[keep.view(-1).long()]\n",
    "            \n",
    "            # bbox pred / gt and iou calculation\n",
    "            pred_bbox_list = get_pred_bbox(pascal_classes[j], cls_dets.cpu().numpy(), 0.9)\n",
    "        \n",
    "            # IoU calculation\n",
    "            if len(pred_bbox_list):\n",
    "                for bbox_obj in pred_bbox_list:\n",
    "                    iou_list.append(get_iou(gt_bbox_list[0]['bbox'], bbox_obj['bbox']))\n",
    "            else:\n",
    "                iou_list.append(0.0)\n",
    "            \n",
    "            if vis:\n",
    "                im2show = vis_bbox(im2show, gt_bbox_list, (0,255,0))\n",
    "                im2show = vis_bbox(im2show, pred_bbox_list, (0,0,255))\n",
    "                \n",
    "                \n",
    "    misc_toc = time.time()\n",
    "    nms_time = misc_toc - misc_tic\n",
    "    \n",
    "    #if webcam_num == -1:\n",
    "    sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\n' \\\n",
    "                       .format(num_images + 1, len(imglist), detect_time, nms_time))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    if vis:\n",
    "        img_output_path = '/home/martin/JupyterLab/output/faster_rcnn_img'\n",
    "        result_path = os.path.join(img_output_path, imglist[num_images][:-4] + \"_det.jpg\")\n",
    "        # PIXEL value conversion \n",
    "        im2show = cv2.convertScaleAbs(im2show, alpha=(255.0))\n",
    "        cv2.imwrite(result_path, im2show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
